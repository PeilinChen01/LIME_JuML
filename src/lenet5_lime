using Flux
using MLDatasets
using DataFrames
using Random
using StatsBase
using Plots
using Lasso
using LinearAlgebra  # Importing the LinearAlgebra package for norm function

# Load the FashionMNIST dataset
train_data = MLDatasets.FashionMNIST(split=:train)
test_data = MLDatasets.FashionMNIST(split=:test)

# Extract data and labels
train_X = Float32.(train_data.features) / 255.0
train_y = Flux.onehotbatch(train_data.targets .+ 1, 1:10)
test_X = Float32.(test_data.features) / 255.0
test_y = Flux.onehotbatch(test_data.targets .+ 1, 1:10)

# Reshape the data to (28, 28, 1, N)
train_X = reshape(train_X, 28, 28, 1, :)
test_X = reshape(test_X, 28, 28, 1, :)

#= This is for lenet5_model1 Define the LeNet-5 model as per your configuration
model = Chain(
    Conv((5, 5), 1 => 6, relu),
    MaxPool((2, 2)),
    Conv((5, 5), 6 => 16, relu),
    MaxPool((2, 2)),
    Conv((3, 3), 16 => 120, relu),  # Adjusted convolution kernel size
    Flux.flatten,
    Dense(120 * 2 * 2, 84, relu),   # Ensuring the input size of the fully connected layer is correct
    Dense(84, 10),
    softmax
)
=#

# Define the LeNet-5 model
model = Chain(
    Conv((5, 5), 1=>6, relu),   # 1 input channel, 6 output channels
    MaxPool((2, 2)),
    Conv((5, 5), 6=>16, relu),  # 6 input channels, 16 output channels
    MaxPool((2, 2)),
    Flux.flatten,
    Dense(256, 120, relu),
    Dense(120, 84, relu),
    Dense(84, 10),              # 10 output classes
    softmax
)

# Load pre-trained model weights (assuming the weights are saved in 'lenet5_model.bson')
using BSON: @load
# This is for lenet5_model1 @load "lenet5_model2.bson" model
@load "../model/lenet5_model2.bson" model


# Select a test instance to explain
x = test_X[:, :, :, 1]
y_true = test_y[:, 1]

# Function to generate perturbed samples
function sample_around_image(x, num_samples=100, noise_level=0.1)
    z = []
    for _ in 1:num_samples
        push!(z, x .+ noise_level * randn(size(x)))
    end
    return stack(z)
end

# Generate perturbed samples
num_samples = 100
Z = sample_around_image(x, num_samples)

# Predict using the trained model
f_Z = model(Z)

# Verify model output
println("Shape of f_Z: ", size(f_Z))

# Extract the predictions for the specific class (e.g., class 1)
f_Z_class = f_Z[1, :]  # Ensure it's a vector of length 100

# Flatten the perturbed samples for linear regression
Z_flat = reshape(Z, num_samples, 28*28)  # Changed to match the expected input format for Lasso

# Ensure the lengths of the columns match the number of samples
println("Number of samples: ", num_samples)
println("Shape of Z_flat: ", size(Z_flat))
println("Length of f_Z_class: ", length(f_Z_class))

# Convert DataFrame to matrix form for Lasso
X = Z_flat
y = convert(Vector{Float64}, f_Z_class)  # Convert y to Float64 to match X

# Print the shapes of X and y
println("Shape of X: ", size(X))
println("Length of y: ", length(y))

# Fit the Lasso model using Lasso.jl
λ = 0.1
lasso_model = fit(LassoPath, X, y, Normal(), IdentityLink())

# Extract the coefficients
β_opt = coef(lasso_model)[2:end]

# Verify the length of β_opt
println("Length of β_opt: ", length(β_opt))

# Ensure the number of coefficients matches the number of pixels
expected_length = 28 * 28
if length(β_opt) != expected_length
    println("Warning: The number of coefficients does not match the expected number of pixels.")
    β_opt = β_opt[1:expected_length]  # Adjusting the length to match the expected number of pixels
end

# Reshape the coefficients to the shape of the original image
β_image = reshape(β_opt, 28, 28)

# Display the original image and the explanation
heatmap(x[:, :, 1], title="Original Image")
heatmap(β_image, title="LIME Explanation with Lasso")
